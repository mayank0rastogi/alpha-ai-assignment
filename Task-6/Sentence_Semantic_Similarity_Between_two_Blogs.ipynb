{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentence Semantic-Similarity Between two Blogs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGsmx4oOR0pB",
        "outputId": "ef87c822-f739-436e-a146-58a4d2a7b215"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khJsUcLIR701",
        "outputId": "e8f70687-9d01-4ab0-8e93-5c32097b0686"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eh3d71YsSFHX",
        "outputId": "061bbbad-b80c-46b1-f231-366da6bc0637"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NU3dDIqWSKYS",
        "outputId": "98242e4d-9b85-4a6b-cd85-b82fb896a471"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtWLpNyzQgq0",
        "outputId": "6f776995-c49f-4431-b5a3-dabb2182bc25"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords,wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy\n",
        "import itertools\n",
        "\n",
        "\n",
        "\n",
        "#str1 = \"These are really fun if you're looking to freshen up your culinary skills. No matter if you're a professional chef or trying to grow past boiling rice, you'll have content designed for your skill set.\"\n",
        "#str2 = \"These are soo much of enjoyement if you're looking to cleanse up your skills. No topic if you're a known chef or trying to grow past boiling rice, you'll have content created for your skillset.\"\n",
        "\n",
        "str1 = \"Ballmer has been vocal in the past warning that Linux is a threat to Microsoft.\"\n",
        "str2 = \"In the memo, Ballmer reiterated the open-source threat to Microsoft.\"\n",
        "\n",
        "#initilializing stop words for english language\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "## created empty lists\n",
        "swr_sent1 = []\n",
        "swr_sent2 = []\n",
        "lemm_sent1 = []\n",
        "lemm_sent2 = []\n",
        "sims = []\n",
        "temp1 = []\n",
        "temp2 = []\n",
        "simi = []\n",
        "final = []\n",
        "same_sent1 = []\n",
        "same_sent2 = []\n",
        "\n",
        "\n",
        "#lemmetizing the words into their root words for this initialize it\n",
        "lemmatizer  =  WordNetLemmatizer()\n",
        "\n",
        "#Tokenizing and removing the Stopwords\n",
        "for words1 in word_tokenize(str1):\n",
        "    if words1 not in stop_words:\n",
        "        if words1.isalnum():#check whether words are alphanumeric only rather than (space)!#%&? etc. \n",
        "            swr_sent1.append(words1)\n",
        "\n",
        "#Lemmatizing-->Root Words\n",
        "for i in swr_sent1:\n",
        "    lemm_sent1.append(lemmatizer.lemmatize(i))\n",
        "    \n",
        "#print(lemm_sentence1)\n",
        "\n",
        "\n",
        "#Tokenizing and removing the Stopwords for sentence 2\n",
        "\n",
        "for words2 in word_tokenize(str2):\n",
        "    if words2 not in stop_words:\n",
        "        if words2.isalnum():\n",
        "            swr_sent2.append(words2)\n",
        "\n",
        "#Lemmatizing-->Root Words for sentence 2\n",
        "\n",
        "for i in swr_sent2:\n",
        "    lemm_sent2.append(lemmatizer.lemmatize(i))\n",
        "\n",
        "#check for Similarity index calculation for each word\n",
        "for word1 in lemm_sent1:\n",
        "    simi =[]\n",
        "    for word2 in lemm_sent2:\n",
        "        sims = []\n",
        "        syns1 = wordnet.synsets(word1)\n",
        "        \n",
        "        syns2 = wordnet.synsets(word2)\n",
        "        for sense1, sense2 in itertools.product(syns1, syns2):\n",
        "            d = wordnet.wup_similarity(sense1, sense2)\n",
        "            if d != None:\n",
        "                sims.append(d)\n",
        "            \n",
        "            #print(sims)\n",
        "    \n",
        "        \n",
        "        if sims != []:        \n",
        "           max_sim = max(sims)\n",
        "           \n",
        "           simi.append(max_sim)\n",
        "             \n",
        "    if simi != []:\n",
        "        max_final = max(simi)\n",
        "        final.append(max_final)\n",
        "\n",
        "\n",
        "\n",
        "sim_idx = numpy.mean(final)\n",
        "storing_sim_idx = round(sim_idx , 2)\n",
        "print(\"Blog 1: \",str1)\n",
        "print(\"Blog 2: \",str2)\n",
        "print(\"Similarity index value : \", storing_sim_idx)\n",
        "\n",
        "if storing_sim_idx>0.8:\n",
        "    print(\"Similar\")\n",
        "elif storing_sim_idx>=0.6:\n",
        "    print(\"Somewhat Similar\")\n",
        "else:\n",
        "    print(\"Not Similar\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Blog 1:  Ballmer has been vocal in the past warning that Linux is a threat to Microsoft.\n",
            "Blog 2:  In the memo, Ballmer reiterated the open-source threat to Microsoft.\n",
            "Similarity index value :  0.64\n",
            "Somewhat Similar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXasoNPzRxAd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}